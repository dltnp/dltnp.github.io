<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0031)http://paul.rutgers.edu/~ngoyal/ -->
<HTML><HEAD><TITLE>E0 306</TITLE>
<META http-equiv=Content-Type content="text/html; charset=windows-1252">

<META content="Dec 14, 2005">
<BODY text=black bgColor=white><FONT face=arial>
<center><hl><FONT color=#3366ff size=+2>
Deep Learning: Theory and Practice (E0 306)</FONT></hl></center>

<p>


<br>
<br>
<strong>Time:</strong>  Tuesdays and Thursdays, 3:30 PM - 5:00 PM
<br>
<strong>Place: </strong> CSA (252 or 254), Indian Institute of Science
<br><br>

<strong>Instructors: <br>
</strong> 	<A href="https://www.microsoft.com/en-us/research/people/amitdesh/"> Amit Deshpande </A><br>
</strong> 	<A href="https://www.microsoft.com/en-us/research/people/navingo/">Navin Goyal </A>, 
			  	email: navin001 followed by @gmail.com, office hours: right after the class <br>

</strong> 	<A href="https://drona.csa.iisc.ac.in/~anand/"> Anand Louis </A><br>
<br><br>



<HR width="100%">
<a name="lectures">
<strong>Lecture 1</strong> (Jan 8) Introduction to the course and recap of statistical learning theory. <br>
For the large part we followed the first few chapters of [SSBD]; <A href="https://github.com/dltnp/dltnp.github.io/blob/master/L1.pdf"> notes </A> (somewhat rough; to be proof-read) <br> <br>

<strong>Lecture 2</strong> (Jan 10) Rademacher complexity (Chapter 26 of [SSBD]) and other methods to bound generalization error; 
begin brief review of neural networks  <br>
<li> Suggested reading: For a quick mathematical introduction to neural networks we recommend Chapters 5 and 7 of the 
<A href="https://web.stanford.edu/~jurafsky/slp3/"> NLP book of Jurafsky--Martin (ignore Section 7.5); 
    this should suffice for our needs at least in the first part of the course </A> <br> </li>
<li>Optional reading: <A href="http://cs231n.github.io/"> These </A>notes are excellent hands-on introduction to neural nets 
    but are not as succinct as the previous reference <br> </li>
<li>You can train simple neural networks on two dimensional data 
    at <A href="https://playground.tensorflow.org/">Google Playground</A>. Great for visualizations </li>
<br><br>




<HR width="100%">

<a name="description">
<strong>Course Description: </strong> 
The area of deep learning has been making rapid empirical advances, 
however this success is largely guided by intuition and trial and error and remains more of 
an art than science. We lack theory that applies "end-to-end." While the traditional theory of 
machine learning leaves much to be desired, current research to remedy this is very active. 
Besides being of interest in its own right, progress on theory has the potential to further 
improve the current deep learning methods. This course will bring students up to date to the 
current fast-moving frontier. Our primary focus will be on theoretical aspects.

<br><br>
Brief tentative list of topics: <br><br>
<li>Recap of statistical learning theory: Rademacher complexity and other generalization bounds</li>

<li>Quick introduction to the basics of neural networks</li>

<li>Generalization in deep learning</li>

<li>Expressive power of neural networks</li>

<li>Adversarial examples</li>

<li>Optimization for deep learning</li>

<li>Generative models</li>



<br><br>
<strong>Prerequisites:</strong> Probability, linear algebra and optimization. Previous exposure to machine learning and deep learning 
will be helpful. 

<br><br>
<strong>Grading:</strong> 
bi-weekly homework assignments (30%), scribe notes (10%), midterm (35%), paper presentation (25%) (a list of papers will be shared by Jan 20)




<br><br>
<strong>Text:</strong> There is no required text for the class. However, the following references will be useful.
<li> [SSBD] <A href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html">Understanding Machine Learning: From Theory to Algorithms by Shalev-Schwartz and Ben-David</A> </li>
<li> [DL] <A href="https://www.deeplearningbook.org/">Deep Learning</A> by Goodfellow, Bengio and Courville
<li> <A = href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html#">High-Dimensional Probability</A> by Vershynin</li>

    






</FONT>
</BODY>
</HTML>

