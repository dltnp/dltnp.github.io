<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0031)http://paul.rutgers.edu/~ngoyal/ -->
<HTML><HEAD><TITLE>E0 306</TITLE>
<META http-equiv=Content-Type content="text/html; charset=windows-1252">

<META content="Dec 14, 2005">
<BODY text=black bgColor=white><FONT face=arial>
<center><hl><FONT color=#3366ff size=+2>
Deep Learning: Theory and Practice (E0 306)</FONT></hl></center>

<p>


<br>
<br>
<strong>Time:</strong>  Tuesdays and Thursdays, 3:30 PM - 5:00 PM
<br>
<strong>Place: </strong> CSA 252, Indian Institute of Science
<br><br>

<strong>Instructors: <br>
</strong> 	<A href="https://www.microsoft.com/en-us/research/people/amitdesh/"> Amit Deshpande </A><br>
</strong> 	<A href="https://www.microsoft.com/en-us/research/people/navingo/">Navin Goyal </A>, 
			  	email: navin001 followed by @gmail.com  <br>

</strong> 	<A href="https://drona.csa.iisc.ac.in/~anand/"> Anand Louis </A><br>
<br><br>



<HR width="100%">
<a name="lectures">
<strong>Lecture 1</strong> (Jan 8) Introduction to the course and recap of statistical learning theory



<br><br>




<HR width="100%">

<a name="description">
<strong>Course Description: </strong> 
The area of deep learning has been making rapid empirical advances, 
however this success is largely guided by intuition and trial and error and remains more of 
an art than science. We lack theory that applies "end-to-end." While the traditional theory of 
machine learning leaves much to be desired, current research to remedy this is very active. 
Besides being of interest in its own right, progress on theory has the potential to further 
improve the current deep learning methods. This course will bring students up to date to the 
current fast-moving frontier. Our primary focus will be on theoretical aspects.

<br><br>
Brief tentative list of topics: <br><br>
<li>Recap of statistical learning theory: Rademacher complexity and other generalization bounds</li>

<li>Quick introduction to the basics of neural networks</li>

<li>Generalization in deep learning</li>

<li>Expressive power of neural networks</li>

<li>Adversarial examples</li>

<li>Optimization for deep learning</li>

<li>Generative models</li>



<br><br>
<strong>Prerequisites:</strong> Probability, linear algebra and optimization. Previous exposure to machine learning and deep learning 
will be helpful. 

<br><br>
<strong>Grading:</strong> There will be bi-weekly homework assignments   

<br><br>
<strong>Text:</strong> There is no required text for the class. However, the following references will be useful.
<li> <A href="http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html">Understanding Machine Learning: From Theory to Algorithms</A> </li>
<li><A href="https://www.deeplearningbook.org/">Deep Learning</A></li>

    






</FONT>
</BODY>
</HTML>

